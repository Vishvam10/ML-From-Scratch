{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a46251a7",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4568c2",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "\n",
    "### Model \n",
    "\n",
    "$$\n",
    "    y = sign(\\textbf w^{T}\\textbf x + b)\n",
    "$$\n",
    "\n",
    "where $\\textbf w$ is the weight vector, $\\textbf x$ is the feature matrix, $b$ is the bias and $sign()$ is the signun function.\n",
    "\n",
    "### Loss ( Hinge Loss ) with L2 Regularization\n",
    "\n",
    "$$\n",
    "    J(\\textbf w, b) = \\frac {1} {2} ||\\textbf w||^2 + C \\sum \\limits_{i=1}^{n} ( max(0, [1 - \\textbf y^{(i)}(\\textbf w^{T}\\textbf x^{(i)} + b)]))\n",
    "$$\n",
    "\n",
    "\n",
    "### Partial Derivative of the Loss Function\n",
    "\n",
    "$$\n",
    "   \\frac {\\partial} {\\partial \\textbf w} J(\\textbf w, b) = \\textbf w + C \\sum \\limits_{i=1}^{n} \\mathbb 1(1 - \\textbf y^{(i)}(\\textbf w^{T}\\textbf x^{(i)} + b) > 0)\\textbf y^{(i)} \\textbf x^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "   \\frac {\\partial} {\\partial b} J(\\textbf w, b) = C \\sum \\limits_{i=1}^{n} \\mathbb 1(1 - \\textbf y^{(i)}(\\textbf w^{T}\\textbf x^{(i)} + b) > 0)\\textbf y^{(i)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82774eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41706515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class softSVM:\n",
    "    def __init__(self, C):\n",
    "        self.support_vectors = None\n",
    "        self.C = C\n",
    "        self.b = None\n",
    "        \n",
    "    def __decision_function(self, X):\n",
    "        return X @ self.w + self.b\n",
    "    \n",
    "    def __loss(self, margin):\n",
    "        return 0.5 * self.w.dot(self.w) + self.C * np.sum(np.maximum(0, 1 - margin))\n",
    "    \n",
    "    def __margin(self, X, y):\n",
    "        return y * self.__decision_function(X)\n",
    "    \n",
    "    def fit(self, X, y, lr=1e-3, epochs=500):\n",
    "        n, d = X.shape\n",
    "        self.w = np.random.randn(d)\n",
    "        self.b = 0\n",
    "        \n",
    "        losses = []\n",
    "        for _ in range(epochs):\n",
    "            margin = self.__margin(X, y)\n",
    "            loss = self.__loss(margin)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            misclassified_pts_idx = np.where(margin < 1)[0]\n",
    "            d_w = self.w - self.C * y[misclassified_pts_idx].dot(X[misclassified_pts_idx])\n",
    "            self.w = self.w - lr * d_w\n",
    "            \n",
    "            d_b = - self.C * np.sum(y[misclassified_pts_idx])\n",
    "            self.b = self.b - lr * d_b\n",
    "            \n",
    "            self.support_vectors = np.where(margin <= 1)[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.sign(self.__decision_function(X))\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        p = self.predict(X)\n",
    "        return np.mean(y == p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c89a4c",
   "metadata": {},
   "source": [
    "Note that usually we calculated other evaluation metrics such as f1-score, precision, recall, etc. instead of mean as mentioned above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cccb24",
   "metadata": {},
   "source": [
    "**Testing the `softSVM()` class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4649ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (100,)\n",
      "[0.32549484 0.27311968] -1.7029999999999719\n",
      "[ 0 52 55 74]\n"
     ]
    }
   ],
   "source": [
    "X, Y = make_blobs(n_samples=100, n_features=2, centers=[[0,0],[6,6]], cluster_std=1, random_state=12)\n",
    "Y = np.where(Y == 0, -1, 1)\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "svm = softSVM(C=1)\n",
    "svm.fit(X, Y)\n",
    "\n",
    "print(svm.w, svm.b)\n",
    "print(svm.support_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffea49",
   "metadata": {},
   "source": [
    "Adding some noise to the data. Also note that **as the value of C increases, them margin gets smaller and value of b increases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d037adc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1 -1 -1 -1  1  1  1  1  1  1 -1  1 -1 -1 -1  1 -1  1  1 -1 -1 -1 -1\n",
      "  1 -1 -1 -1 -1 -1  1  1  1 -1 -1 -1  1 -1  1  1  1 -1  1 -1 -1 -1  1  1\n",
      "  1  1  1 -1  1 -1 -1  1  1 -1  1  1]\n",
      "[ 2.9132867  -7.72155226] 12.200000000000012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = make_blobs(n_samples=60, n_features=2, centers=2, cluster_std=1.1, random_state=0)\n",
    "Y = np.where(Y == 0, -1, 1)\n",
    "\n",
    "print(Y)\n",
    "\n",
    "svm = softSVM(C=100)  \n",
    "svm.fit(X, Y)\n",
    "\n",
    "print(svm.w, svm.b)\n",
    "\n",
    "newExample = np.array([-2.2,2.2])\n",
    "svm.predict(newExample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d5bacc",
   "metadata": {},
   "source": [
    "**`softSVM()` will not work well if you have non-linear decision boundaries. In such cases, use `kernelSVM()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0700e6",
   "metadata": {},
   "source": [
    "## Kernel SVM\n",
    "\n",
    "From $\\mathcal{L}_d$, the ( Wolfe ) dual for soft SVM is as follows :\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}_d(\\alpha) = \\sum \\limits_{i=1}^{n} \\alpha^{(i)} - \\frac {1} {2} \\sum \\limits_{i=1}^{n} \\sum \\limits_{k=1}^{n} \\langle \\alpha^{(i)} \\textbf y^{(i)} \\textbf x^{(i)}, \\alpha^{(k)}, \\textbf y^{(k)}, \\textbf x^{(k)} \\rangle\n",
    "$$\n",
    "\n",
    "where $\\langle u,v \\rangle$ is the inner product ( here, dot product )\n",
    "\n",
    "**Subject to constrains $\\forall i \\in 1...n$ :**\n",
    "- $0 \\le \\alpha^{(i)} \\le C$\n",
    "- $\\sum \\limits_{i=1}^{n} \\alpha^{(i)} \\textbf y^{(i)} = 0$\n",
    "\n",
    "\n",
    "### Kernels\n",
    "\n",
    "**Linear Kernel** :\n",
    "$$\n",
    "    K(\\textbf x^{i}, \\textbf x^{(j)}) = \\textbf x^{(i)^T} \\textbf x^{(j)}\n",
    "$$\n",
    "\n",
    "**Polynomial Kernel** :\n",
    "$$\n",
    "    K(\\textbf x^{i}, \\textbf x^{(j)}) = \\left ( 1 + \\textbf x^{(i)^T} \\textbf x^{(j)} \\right )^{d}\n",
    "$$\n",
    "\n",
    "**Radial Basis Functions (RBF) Kernel** :\n",
    "$$\n",
    "    K(\\textbf x^{i}, \\textbf x^{(j)}) = e^{\\left ( - \\frac {(\\textbf x^{(i)} - \\textbf x^{(j)})^2} {\\sigma^2} \\right )}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a8155bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kernelSVM():\n",
    "    def __init__(self, C=0, kernel='rbf', sigma=0.1, degree=2):\n",
    "        self.C = C\n",
    "        if kernel == 'poly':\n",
    "            self.kernel = self.__polynomial_kernel\n",
    "            self.degree = degree\n",
    "        else:\n",
    "            self.kernel = self.__rbf_kernel\n",
    "            self.sigma = sigma\n",
    "        self.alpha = None\n",
    "        self.b = 0\n",
    "    \n",
    "    def __rbf_kernel(self, X1, X2):\n",
    "        return np.exp(-(1/self.sigma ** 2) * np.linalg.norm(X1[:, np.newaxis] - X2[np.newaxis, :], axis=2) ** 2)\n",
    "    \n",
    "    def __polynomial_kernel(self, X1, X2):\n",
    "        return (self.C + X1.dot(X2.T)) ** self.degree\n",
    "    \n",
    "    def fit(self, X, y, lr=1e-3, epochs=100):\n",
    "        self.alpha = np.random.random(X.shape[0])\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.b = 0\n",
    "        y_iy_kx_ix_k = np.outer(y, y) * self.kernel(X, X)\n",
    "        \n",
    "        losses = []\n",
    "        for _ in range(epochs):\n",
    "            gradient = np.ones(X.shape[0]) - y_iy_kx_ix_k.dot(self.alpha)\n",
    "            self.alpha += lr * gradient\n",
    "            self.alpha[self.alpha > self.C] = self.C\n",
    "            self.alpha[self.alpha < 0] = 0\n",
    "            \n",
    "            loss = np.sum(self.alpha) - 0.5 * np.sum(np.outer(self.alpha, self.alpha) * y_iy_kx_ix_k)\n",
    "            losses.append(loss)\n",
    "        index = np.where((self.alpha > 0) & (self.alpha < self.C))[0]\n",
    "        b_i = y[index] - (self.alpha * y).dot(self.kernel(X, X[index]))\n",
    "        self.b = np.mean(b_i)\n",
    "    \n",
    "    def __decision_function(self, X):\n",
    "        return (self.alpha * self.y).dot(self.kernel(self.X, X)) + self.b\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.sign(self.__decision_function(X))\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        return np.maen(y == y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e1fda3",
   "metadata": {},
   "source": [
    "**Testing the `kernelSVM()` class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b00bb355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "-1.3634304244320172\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(500, factor=0.5, noise=0.08)\n",
    "y[y == 0] == -1\n",
    "\n",
    "svm = kernelSVM(C=1.0, kernel='poly', degree=2)\n",
    "svm.fit(X, y, lr=1e-3)\n",
    "\n",
    "print(svm.C)\n",
    "print(svm.b)\n",
    "\n",
    "# print(svm.alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
