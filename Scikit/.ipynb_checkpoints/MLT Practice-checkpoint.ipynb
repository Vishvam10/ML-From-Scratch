{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb25f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04fbf5",
   "metadata": {},
   "source": [
    "## Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7d83f8",
   "metadata": {},
   "source": [
    "**PQ1**\n",
    "\n",
    "Define a function cross_entropy(y,sigmoid_vector,w,reg_type,reg_rate) having the \n",
    "following characteristics: \n",
    "\n",
    "Input:\n",
    "\n",
    "    y: Actual output label vector\n",
    "    sigmoid_vector: logistic value of predicted output \n",
    "    w: weight vector\n",
    "    reg_type: type of regularization as string, either 'l1' or 'l2'. Default 'l2'.\n",
    "    reg_rate: regularization rate. Default value 0.\n",
    "\n",
    "Output:\n",
    "\n",
    "    Binary cross entropy loss(float value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089b5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y,sigmoid_vector,w,reg_type,reg_rate) :\n",
    "    if(reg_type == \"l1\") :\n",
    "        return np.sum(y * np.log(sigmoid_vector) + (1 -  y) * (np.log(1 - sigmoid_vector))) + (reg_rate * np.sum(np.abs(w)))\n",
    "    else :\n",
    "        return np.sum(y * np.log(sigmoid_vector) + (1 -  y) * (np.log(1 - sigmoid_vector))) + (reg_rate * np.sum(np.dot(w.T, w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae5f6d",
   "metadata": {},
   "source": [
    "**GQ1** \n",
    "\n",
    "Assume that we have trained a logistic regression classifier on a dataset and have learned the weight w. Define a function predict_label(X,w) which accepts a feature matrix X of test samples and the weight vector w as arguments, and assigns labels to each of the samples based on the following conditions:\n",
    "\n",
    "- If the model's output is greater than or equal to 0.75, assign the predicted label as 1\n",
    "- If the model's output is less than or equal to 0.25, assign the predicted label as -1\n",
    "- Otherwise, assign the label as 0\n",
    "\n",
    "The function should return the vector of predicted labels. Use the sigmoid activation function while calculating the model's output for all the sample values in the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "046a2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(X, w) :\n",
    "    y_hat = 1/(1+np.exp(-(X @ w)))\n",
    "    labels=[]\n",
    "    # compute label of the sample\n",
    "    for y in y_hat:\n",
    "        if y<=0.25:\n",
    "            label=-1\n",
    "        elif y>=0.75:\n",
    "            label=1\n",
    "        else:\n",
    "            label=0\n",
    "        labels.append(label)\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4970e",
   "metadata": {},
   "source": [
    "**GQ2**\n",
    "\n",
    "Define a function gradient(X, y, w, reg_rate) which can be used for optimization of logistic regression model with L2 regularization having the following characteristics:\n",
    "Input:\n",
    "\n",
    "    X: Feature matrix for training data.\n",
    "    y: Label vector for training data.\n",
    "    reg_rate: regularization rate\n",
    "    w: weight_vector\n",
    "\n",
    "Output:\n",
    "\n",
    "     A vector of gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9831f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, w, reg_rate) :\n",
    "    grad = X.T @ (sigmoid(X @ w) - y) + (reg_rate * w)\n",
    "\n",
    "def sigmoid(self, z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9671c4",
   "metadata": {},
   "source": [
    "**GQ3**\n",
    "\n",
    "Define a function update_w(X, y, w, reg_rate, lr) which can be used for optimization of logistic regression model with L2 regularization having following characteristics:\n",
    "Input:\n",
    "\n",
    "    X: Feature matrix for training data.\n",
    "    y: Label vector for training data.\n",
    "    reg_rate: regularization rate\n",
    "    w: weight_vector\n",
    "    lr: learning rate\n",
    "\n",
    "Output:\n",
    "\n",
    "     A vector of updated weights.\n",
    "\n",
    "You need to perform exactly one update over the entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc87678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w(X, y, w, reg_rate, lr) :\n",
    "    grad = X.T @ (sigmoid(X @ w) - y) + (reg_rate * w)\n",
    "    w = w - lr * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe66b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression() :\n",
    "    \n",
    "    def __init__(self, w) :\n",
    "        self.w = w\n",
    "        \n",
    "    def sigmoid(z) :\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def linear_combination(self, X) :\n",
    "        return X @ self.w\n",
    "\n",
    "    def activation(self, X) :\n",
    "        return self.sigmoid(self.linear_combination(X))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5) :\n",
    "        return np.where(self.activation(X) > threshold, 1, 0)\n",
    "    \n",
    "    def calucate_gradient(self, X, y, reg_rate) :\n",
    "        return X.T @ (self.sigmoid(X @ w) - y) + (reg_rate * w)\n",
    "            \n",
    "    def update_weights(self, X, y, reg_rate, lr) :\n",
    "        grad = calculate_gradient(X, y, reg_rate)\n",
    "        self.w = self.w - lr *  grad\n",
    "        \n",
    "    def loss(self, X, y, reg_rate) :\n",
    "        sigmoid_vector = self.activation(X)\n",
    "        l = np.sum((y * np.log(sigmoid_vector) + (1 - y) * (np.log(1 - sigmoid_vector)))) + \n",
    "        r = 0.5 * (reg_rate * np.sum(np.dot(w.T, w)))\n",
    "        l += r\n",
    "        return l\n",
    "        \n",
    "    def gd(self, X, y, epochs, reg_rate, lr) :\n",
    "        self.w = np.zeros(y.shape[0])\n",
    "        self.w_all = []\n",
    "        self.l_all = []\n",
    "        \n",
    "        for e in epochs :\n",
    "            self.w = self.caclulate_gradient(X, y, reg_rate)\n",
    "            self.w_all.append(self.w)\n",
    "            self.l_all.append(self.loss(X, y, reg_rate))\n",
    "            self.w = self.update_weights(X, y, reg_rate, lr)\n",
    "\n",
    "        return self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188747f2",
   "metadata": {},
   "source": [
    "## Week 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a9525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeeef66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a73ea35b",
   "metadata": {},
   "source": [
    "## Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ee401",
   "metadata": {},
   "source": [
    "**PQ1**\n",
    "\n",
    "Write a function euclid(a,b) to find Euclidean distance between vectors a and b. Both a and b have shape (n,1), where n is number of features/dimensions.\n",
    "\n",
    "Input: Vectors a and b\n",
    "Output: Euclidean distance between vectors a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca51c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid(a,b) :\n",
    "    return np.sqrt((a-b)**2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1bbc91",
   "metadata": {},
   "source": [
    "**PQ2**\n",
    "\n",
    "Write a function **one_hot(y)** which performs one hot encoding on vector y and then outputs a resultant matrix which can be used for softmax regression as output label matrix. y is row matrix with (n,1) shape, where n is number of samples.\n",
    "Example: if y is [8, 6, 3], its one hot encoding will be [[0, 0, 1],[0, 1, 0],[1, 0, 0]].\n",
    "\n",
    "Input: y: A vector of shape (n,1)\n",
    "Output: A output label matrix of suitable shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59457bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot(y):\n",
    "    val = set(y)\n",
    "    ei = np.eye(max(val)+1)\n",
    "\n",
    "    return ei[y]\n",
    "\n",
    "a= np.array([1,2,3,5,2,2,4,5,1,3])\n",
    "print(one_hot(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee03f3b",
   "metadata": {},
   "source": [
    "**GQ1**\n",
    "Write a function manhattan(a,b) to find Manhattan distance between vectors a and b. Both a and b are vectors with shape (n,1) where n is number of features/dimensions.\n",
    "\n",
    "Input: Vectors a and b\n",
    "Output: Manhattan distance between vectors a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a00f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan(a,b) :\n",
    "    return np.sum(np.abs(a-b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae970e1c",
   "metadata": {},
   "source": [
    "**GQ2**\n",
    "\n",
    "Write a function softmax(Z) to find softmax of linear combination of feature matrix and weight vector.\n",
    "Take care of numerical stability as well.\n",
    "\n",
    "Input: Z = X@W, where X is feature matrix of shape (n,m), W is weight matrix (m,k), where n = number of rows, m = number of features and k = number of labels.\n",
    "\n",
    "Output: A matrix of (n,1) shape. Each row corresponds to the label of that row. Each label will have value from 0 to k-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "566d3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z) :\n",
    "    s = np.exp(Z) / (np.sum(np.exp(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a4b74",
   "metadata": {},
   "source": [
    "**GQ3**\n",
    "\n",
    "Write a function knn(class1, class2, x_new) to find in which cluster x_new belongs using 3-NN. Assume cluster 1 and cluster 2 has 5 points each. Use Euclidian distance as distance measure.\n",
    "\n",
    "Input: Two numpy arrays class1 and class2 of shape (5,2) each and a numpy array x_new of shape (2,1)\n",
    "Output: Return class id to which x_new belongs (i.e. 1 or 2) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7939a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 6]\n",
      " [5 8]\n",
      " [3 8]\n",
      " [2 4]\n",
      " [3 3]\n",
      " [4 2]]\n",
      "All points are  {9: 1, 13: 1, 5: 1, 4: 2, 10: 2, 20: 2}\n",
      "Points nearest to the new examples are [(4, 2), (5, 1), (9, 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(4, 2), (5, 1), (9, 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def euclid(a,b) :\n",
    "    return np.sum((a-b)**2, axis=1).tolist()\n",
    "\n",
    "def knn(class1, class2, x_new) :\n",
    "    c = np.concatenate((class1,class2), axis=0)\n",
    "    cl = []\n",
    "    for i in range(2 * len(class1)) :\n",
    "        if(i < len(class1)) :\n",
    "            cl.append(1)\n",
    "        else :\n",
    "            cl.append(2)\n",
    "    print(c)\n",
    "    points = dict(zip(euclid(c, x_new), cl))    \n",
    "    print(\"All points are \", points)\n",
    "\n",
    "    least_3 = sorted(points.items())[:3]\n",
    "    print(f\"Points nearest to the new examples are {least_3}\")\n",
    "    \n",
    "    return least_3\n",
    "\n",
    "X1 = np.array([[5, 6],[5, 8],[3, 8]]) \n",
    "X2 = np.array([[2,4], [3,3], [4,2]])\n",
    "ne = np.array([2,6])\n",
    "\n",
    "knn(X1, X2, ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d19bb6",
   "metadata": {},
   "source": [
    "## Week 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a670c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperplane_value(x,w,b, offset):\n",
    "      return -1 * (w[0] * x + b + offset) / w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd411c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fit_softSVM:\n",
    "    def __init__(self, C=500):\n",
    "        \n",
    "        self._support_vectors = None \n",
    "        self.C = C \n",
    "        self.w = None \n",
    "        self.b = None \n",
    "\n",
    "        # n is the number of data points \n",
    "        self.n = 0\n",
    "        # d is the number of dimensions \n",
    "        self.d = 0\n",
    "\n",
    "    def _decision_function(self, X):\n",
    "        return X.dot (self.w) + self.b\n",
    "      \n",
    "    def _cost(self, margin):\n",
    "        return (1 / 2) * self.w.dot (self.w) + self.C * np.sum(np.maximum(0, 1 - margin))\n",
    " \n",
    "    def _margin(self, X, y):\n",
    "        return y * self._decision_function(X)\n",
    " \n",
    "    def fit(self, X, y, lr=0.001, epochs=100):\n",
    "        self.n, self.d = X.shape\n",
    "        self.w = np.random.randn(self.d) \n",
    "        self.b = 0\n",
    "        \n",
    "        loss_array = []\n",
    "        for _ in range ( epochs):\n",
    "            margin = self._margin(X, y) \n",
    "            loss = self._cost(margin) \n",
    "            loss_array.append(loss)\n",
    "            \n",
    "            misclassified_pts_idx = np.where(margin < 1)[0]\n",
    "            d_w = self.w - self.C * y[misclassified_pts_idx].dot(X[misclassified_pts_idx])\n",
    "            self.w = self.w - lr * d_w\n",
    "        \n",
    "            d_b = - self.C * np.sum(y[misclassified_pts_idx])\n",
    "            self.b = self.b - lr * d_b\n",
    "        \n",
    "        self._support_vectors = np.where(self._margin(X, y) <= 1)[0]\n",
    "        return self._support_vectors\n",
    "\n",
    "def support_vectors(X_train, y_train):\n",
    "    softSVM = fit_softSVM()\n",
    "    support_vectors = softSVM.fit(X_train,y_train)\n",
    "\n",
    "    return support_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ec3bd",
   "metadata": {},
   "source": [
    "## Week 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345d165",
   "metadata": {},
   "source": [
    "**PQ1**\n",
    "\n",
    "Consider a regression problem with feature matrix X with size (100, 10) and label vector y with size (100, 1). We split this root node into two nodes 'node1' and 'node2' according to jth split variable and split value 's'.\n",
    "Define a function `predict_node(X, y, j, s)` that takes X, y, split variable j and split value s as parameters and returns the tuple of predict values (mean value) at both the nodes.\n",
    "\n",
    "X = ndarray of size (100, 10) with entries as float.\n",
    "\n",
    "y = ndarray of size (100, 1) with entries as float.\n",
    "\n",
    "j = int\n",
    "\n",
    "s = float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6496895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_node(X, y, j, s):\n",
    "    sep = X[:, j]\n",
    "    region1 = y[np.where(sep <= s)]\n",
    "    region2 = y[np.where(sep > s)]\n",
    "    \n",
    "    m1 = np.mean(region1)\n",
    "    m2 = np.mean(region2)\n",
    "    \n",
    "    return (m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eef537",
   "metadata": {},
   "source": [
    "**PQ2**\n",
    "\n",
    "Define a function `predict_class(y)` that takes the parameter\n",
    "`y` which is a ndarray of actual outputs of all the samples in a particualar node and retruns the predict class for the same node.\n",
    "\n",
    "Note: number of classes are 10 (0 to 9). \n",
    "\n",
    "If two classes have same number of samples, function should return the lower class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29314c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(y):\n",
    "    classes = np.unique(y)\n",
    "    probs = []\n",
    "    for c in classes :\n",
    "        p = len(np.where(y == c)[0]) / len(y)\n",
    "        probs.append(p)\n",
    "    \n",
    "    pred = classes[np.argmax(probs)]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90472b",
   "metadata": {},
   "source": [
    "**PQ3**\n",
    "\n",
    "Define a function `misclassification_error(y)`  that takes the parameter `y` which is a ndarray of actual outputs of all the samples in a particular node and return the misclassification error of the same node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e19e761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassification_error(y):\n",
    "    classes = np.unique(y)\n",
    "    probs = []\n",
    "    for c in classes :\n",
    "        p = len(np.where(y == c)[0]) / len(y)\n",
    "        probs.append(p)\n",
    "    return 1 - np.max(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e0e0c5",
   "metadata": {},
   "source": [
    "**GQ1**\n",
    "\n",
    "Define a function `gini_index(dict)` having the \n",
    "following characteristics:\n",
    "\n",
    "Input:\n",
    "`dict` = dictionary that has classes as keys and 'number of samples in respective class' as values of a particular node.\n",
    "\n",
    "Output:\n",
    "Gini index of the same node. (A float value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfc467ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(data):\n",
    "    n = sum(data.values())\n",
    "    ans = 0\n",
    "    for k, v in data.items() :\n",
    "        p = v / n\n",
    "        ans += (p * (1 - p))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d31bf45",
   "metadata": {},
   "source": [
    "**GQ2**\n",
    "\n",
    "Define a function `entropy(dict)` having the \n",
    "following characteristics:\n",
    "\n",
    "Input:\n",
    "`dict` = dictionary that has classes as keys and 'number of samples in respective class' as values of a particular node.\n",
    "\n",
    "Output:\n",
    "Entropy of the same node. (A float value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb7436b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data) :\n",
    "    n = sum(dict.values())\n",
    "    ans = 0\n",
    "    for value in dict.values():\n",
    "        ans += -(value/n)*np.log2(value/n)\n",
    "    return ans  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c5cd3",
   "metadata": {},
   "source": [
    "**GQ3**\n",
    "\n",
    "Consider a regression problem using CART. If y is a ndarray of targets of the samples in a particular node, define a function `sseloss(y)` that returns the error associated with that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sseloss(y) :\n",
    "    avg = np.mean(y)\n",
    "    res = y - avg\n",
    "    return np.sum((res ** 2))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba0b5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36458d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897093a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b27d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca2a40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e38293cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1     2     3     4     5    12     3]\n",
      " [ 1123  2435    36    47   586  5612    32]\n",
      " [ 1234   241   635  4765  5678   132   343]\n",
      " [ 1345   267  5368    84    95 23127   398]]\n",
      "[   5  586 5678   95]\n",
      "[    3     4     5    12     3  1123  2435    36    47   586  5612    32\n",
      "  1234   241   635  4765  5678   132   343  1345   267  5368    84    95\n",
      " 23127   398]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4,5,12,3], [1123,2435,36,47,586,5612,32], [1234,241,635,4765,5678,132,343], [1345,267,5368,84,95,23127,398]])\n",
    "print(a)\n",
    "print(a[:, 4])\n",
    "# print(a[a>2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a673c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69f01a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
