{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb25f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04fbf5",
   "metadata": {},
   "source": [
    "## Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7d83f8",
   "metadata": {},
   "source": [
    "**PQ1**\n",
    "\n",
    "Define a function cross_entropy(y,sigmoid_vector,w,reg_type,reg_rate) having the \n",
    "following characteristics: \n",
    "\n",
    "Input:\n",
    "\n",
    "    y: Actual output label vector\n",
    "    sigmoid_vector: logistic value of predicted output \n",
    "    w: weight vector\n",
    "    reg_type: type of regularization as string, either 'l1' or 'l2'. Default 'l2'.\n",
    "    reg_rate: regularization rate. Default value 0.\n",
    "\n",
    "Output:\n",
    "\n",
    "    Binary cross entropy loss(float value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y,sigmoid_vector,w,reg_type,reg_rate) :\n",
    "    if(reg_type == \"l1\") :\n",
    "        return np.sum(y * np.log(sigmoid_vector) + (1 -  y) * (np.log(1 - sigmoid_vector))) + (reg_rate * np.sum(np.abs(w)))\n",
    "    else :\n",
    "        return np.sum(y * np.log(sigmoid_vector) + (1 -  y) * (np.log(1 - sigmoid_vector))) + (reg_rate * np.sum(np.dot(w.T, w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae5f6d",
   "metadata": {},
   "source": [
    "**GQ1** \n",
    "\n",
    "Assume that we have trained a logistic regression classifier on a dataset and have learned the weight w. Define a function predict_label(X,w) which accepts a feature matrix X of test samples and the weight vector w as arguments, and assigns labels to each of the samples based on the following conditions:\n",
    "\n",
    "- If the model's output is greater than or equal to 0.75, assign the predicted label as 1\n",
    "- If the model's output is less than or equal to 0.25, assign the predicted label as -1\n",
    "- Otherwise, assign the label as 0\n",
    "\n",
    "The function should return the vector of predicted labels. Use the sigmoid activation function while calculating the model's output for all the sample values in the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(X, w) :\n",
    "    y_hat = 1/(1+np.exp(-(X @ w)))\n",
    "    labels=[]\n",
    "    # compute label of the sample\n",
    "    for y in y_hat:\n",
    "        if y<=0.25:\n",
    "            label=-1\n",
    "        elif y>=0.75:\n",
    "            label=1\n",
    "        else:\n",
    "            label=0\n",
    "        labels.append(label)\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4970e",
   "metadata": {},
   "source": [
    "**GQ2**\n",
    "\n",
    "Define a function gradient(X, y, w, reg_rate) which can be used for optimization of logistic regression model with L2 regularization having the following characteristics:\n",
    "Input:\n",
    "\n",
    "    X: Feature matrix for training data.\n",
    "    y: Label vector for training data.\n",
    "    reg_rate: regularization rate\n",
    "    w: weight_vector\n",
    "\n",
    "Output:\n",
    "\n",
    "     A vector of gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, w, reg_rate) :\n",
    "    grad = X.T @ (sigmoid(X @ w) - y) + (reg_rate * w)\n",
    "\n",
    "def sigmoid(self, z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9671c4",
   "metadata": {},
   "source": [
    "**GQ3**\n",
    "\n",
    "Define a function update_w(X, y, w, reg_rate, lr) which can be used for optimization of logistic regression model with L2 regularization having following characteristics:\n",
    "Input:\n",
    "\n",
    "    X: Feature matrix for training data.\n",
    "    y: Label vector for training data.\n",
    "    reg_rate: regularization rate\n",
    "    w: weight_vector\n",
    "    lr: learning rate\n",
    "\n",
    "Output:\n",
    "\n",
    "     A vector of updated weights.\n",
    "\n",
    "You need to perform exactly one update over the entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc87678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w(X, y, w, reg_rate, lr) :\n",
    "    grad = X.T @ (sigmoid(X @ w) - y) + (reg_rate * w)\n",
    "    w = w - lr * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe66b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression() :\n",
    "    \n",
    "    def __init__(self, w) :\n",
    "        self.w = w\n",
    "        \n",
    "    def sigmoid(z) :\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def linear_combination(self, X) :\n",
    "        return X @ self.w\n",
    "\n",
    "    def activation(self, X) :\n",
    "        return self.sigmoid(self.linear_combination(X))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5) :\n",
    "        return np.where(self.activation(X) > threshold, 1, 0)\n",
    "    \n",
    "    def calucate_gradient(self, X, y, reg_rate) :\n",
    "        return X.T @ (self.sigmoid(X @ w) - y) + (reg_rate * w)\n",
    "            \n",
    "    def update_weights(self, X, y, reg_rate, lr) :\n",
    "        grad = calculate_gradient(X, y, reg_rate)\n",
    "        self.w = self.w - lr *  grad\n",
    "        \n",
    "    def loss(self, X, y, reg_rate) :\n",
    "        sigmoid_vector = self.activation(X)\n",
    "        l = np.sum((y * np.log(sigmoid_vector) + (1 - y) * (np.log(1 - sigmoid_vector)))) + \n",
    "        r = 0.5 * (reg_rate * np.sum(np.dot(w.T, w)))\n",
    "        l += r\n",
    "        return l\n",
    "        \n",
    "    def gd(self, X, y, epochs, reg_rate, lr) :\n",
    "        self.w = np.zeros(y.shape[0])\n",
    "        self.w_all = []\n",
    "        self.l_all = []\n",
    "        \n",
    "        for e in epochs :\n",
    "            self.w = self.caclulate_gradient(X, y, reg_rate)\n",
    "            self.w_all.append(self.w)\n",
    "            self.l_all.append(self.loss(X, y, reg_rate))\n",
    "            self.w = self.update_weights(X, y, reg_rate, lr)\n",
    "\n",
    "        return self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188747f2",
   "metadata": {},
   "source": [
    "## Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4095b5ec",
   "metadata": {},
   "source": [
    "**PQ1**\n",
    "\n",
    "or a binary classification problem, where the feature vector x = (x1,x2) is a two-dimensional binary vector, i.e., each feature is binary. The class label y is indexed using 1 and 2. \n",
    "Assume that the features are conditionally independent given the class labels. Train a Bernoulli Naive-Bayes classifier for this data. Specifically, estimate the following parameter matrix:\n",
    "P = \\begin{bmatrix} p_{11} & p_{12}\\\\ p_{21} & p_{22} \\end{bmatrix}\n",
    "\n",
    "This matrix is to be understood as follows. For features x1 and x2\n",
    "$$\n",
    "    p_{ij} = P(x_{i} = 1 ∣ y = j)\n",
    "$$\n",
    "\n",
    "In $p_{ij}$, the first index stands for the feature and the second stands for the class-label.\n",
    "\n",
    "Write a function named bernoulli_naive_bayes that accepts a feature matrix X and a label vector y as arguments. It should return the parameter matrix P. Both the arguments and the return value are of type np.ndarray. You can assume that no smoothing is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1535a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_naive_bayes(X, y):\n",
    "    \n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "    \n",
    "    p11 = len(np.where((x1 == 1) & (y == 1))[0]) / len(np.where(y == 1)[0])\n",
    "    p12 = len(np.where((x1 == 1) & (y == 2))[0]) / len(np.where(y == 2)[0])\n",
    "    p21 = len(np.where((x2 == 1) & (y == 1))[0]) / len(np.where(y == 1)[0])\n",
    "    p22 = len(np.where((x2 == 1) & (y == 2))[0]) / len(np.where(y == 2)[0])\n",
    "    \n",
    "    P = np.array([[p11, p12], [p21, p22]])\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec91173",
   "metadata": {},
   "source": [
    "**PQ2**\n",
    "\n",
    "You are given a numerical data matrix xxx as an np.ndarray (shape 200×5) and a vector of class labels yyy (size 200) as np.ndarray for a multi-class classification problem. Define a function mean_estimate which calculates the estimated mean of data samples corresponding to the class labels for each feature and returns a dictionary with class labels as keys and estimated mean vectors as values. The ith element of a mean vector corresponds to the ith feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_estimate(X: np.ndarray,  y : np.ndarray):\n",
    "    D = {}\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        D[label] = np.mean(X[y == label], axis = 0)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7babfba0",
   "metadata": {},
   "source": [
    "**PQ4**\n",
    "\n",
    "For a binary classification problem with class labels (0 and 1), define a function class_scores that accepts the true and predicted labels and returns the following evaluation metrics as a dictionary.\n",
    "(1) Precision\n",
    "(2) Recall\n",
    "(3) Accuracy\n",
    "(4) F1 score\n",
    "(5) Misclassification Rate\n",
    "They keys of the dictionary are the names of the metrics, exactly as they are given above. The values are the corresponding measurements expressed as floats. The function should have the following signature:\n",
    "\n",
    "Arguments:  \n",
    "    y_test: true labels, (n, ), np.ndarray \n",
    "    y_pred: predicted labels, (n, ), np.ndarray\n",
    "Return:\n",
    "    metrics: dictionary\n",
    "        key: string, names of the metrics\n",
    "        value: float\n",
    "\n",
    "Both numpy arrays are of size (n,)(n, )(n,). Do not use any existing methods/functions to calculate the same. Consider label 1 as the positive class. Note that the misclassification rate is 1 minus the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_scores(y_test: np.ndarray, y_pred: np.ndarray):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(0, len(y_test)) :\n",
    "        if(y_test[i] == 1 and y_pred[i] == 1) :\n",
    "            tp += 1\n",
    "        if(y_test[i] == 0 and y_pred[i] == 1) :\n",
    "            fp += 1\n",
    "        if(y_test[i] == 0 and y_pred[i] == 0) :\n",
    "            tn += 1\n",
    "        if(y_test[i] == 1 and y_pred[i] == 0) :\n",
    "            fn += 1\n",
    "        \n",
    "    acc = (tp + tn) / (tp + fp + fn + tn)\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f1 = (2 * prec * rec) / (prec + rec)\n",
    "    misclf_rate = 1 - acc\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\" : acc,\n",
    "        \"Precision\" : prec,\n",
    "        \"Recall\" : rec,\n",
    "        \"F1 Score\" : f1,\n",
    "        \"Misclassification Rate\" : misclf_rate\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a069ba4",
   "metadata": {},
   "source": [
    "**GQ1**\n",
    "\n",
    "In a multi-class classification setting, consider a numerical feature matrix XXX as an np.ndarray of shape (n,m)(n, m)(n,m) and a vector of class labels yyy of size (n,)(n, )(n,) as an np.ndarray. Define a function variance_estimate which calculates the estimated variance of data samples corresponding to individual class labels for each feature. The function should return a dictionary with class labels as keys and estimated variance vectors as values. The ith element of a vector corresponds to the variance of the ith feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_estimate(X: np.ndarray,  y: np.ndarray):\n",
    "    D = {}\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        D[label] = np.var(X[y == label], axis = 0)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33cfe82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNB() :\n",
    "    def __init__(self, alpha=1.0) :\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y) :\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        self.w = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self.w_priors = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for c in range(n_classes) :\n",
    "            X_c = X[y == c]\n",
    "            self.w[c: ,] = (np.sum(X_c, axis=0) + self.alpha) / (X_c.shape[0] + n_classes * self.alpha)\n",
    "            self.w_priors[c] = (X_c.shape[0] + self.alpha) / (float(n_samples) + n_classes * self.alpha)\n",
    "\n",
    "            print(\"\\nClass Conditional Density : \\n\", self.w)\n",
    "            print(\"\\nPrior : \\n\", self.w_priors)\n",
    "\n",
    "        return self.w, self.w_priors\n",
    "    \n",
    "    def log_likelihood_prior_prod(self, X) :\n",
    "        return X @ np.log(self.w.T) + (1 - X) @ np.log(1 - self.w.T) + np.log(self.w_priors.T)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        q = self.log_likelihood_prior_prod(X)\n",
    "        return np.argmax(q, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        q = self.log_likelihood_prior_prod(X)\n",
    "        return np.exp(q) / np.expand_dims(np.sum(np.exp(q), axis=1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4dcb06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,0],[0,1],[0,1],[1,0]])\n",
    "y = np.array([1,0,0,1])\n",
    "\n",
    "bernoulli_nb = BernoulliNB()\n",
    "print(\"FIT : \")\n",
    "print(bernoulli_nb.fit(X, y))\n",
    "print(\"\\nPREDICT : \")\n",
    "print(bernoulli_nb.predict(X)) # Note the prediction is matching with the input labels\n",
    "print(\"\\nLOG LIKELIHOOD PRIOR PROD : \")\n",
    "print(bernoulli_nb.log_likelihood_prior_prod(X))\n",
    "print(\"\\nPREDICTED PROBABILITIES : \")\n",
    "print(bernoulli_nb.predict_proba(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04937ab9",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "    \\mu_r = \\frac {1}{n_r}\\sum\\limits_{i=1}^{n} \\mathbb{1}(y^{(i)} = y_r)\\textbf x^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\Sigma_{jjr}^{2} = \\frac {1}{n_r}\\sum\\limits_{i=1}^{n} \\mathbb{1}(y^{(i)} = y_r)(\\textbf x^{(i)} - \\mu_r)^{2}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "    p(x_j | y_c; \\mu_c, \\Sigma_c) = \\frac {1} {(2\\pi)^{d/2}(|\\Sigma_c|)^{1/2}} e^{\\frac {1} {2} (\\textbf x - \\mu_c)^{T}\\Sigma_c^{-1}(\\textbf x - \\mu_c)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8526e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB :\n",
    "    \n",
    "    def __init__(self, alpha=1.0) :\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def fit(self, X, y) :\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "        \n",
    "        for ind, c in enumerate(self._classes) :\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            self._mean[ind, :] = X_c.mean(axis=0)\n",
    "            self._var[c] = X_c.var(axis=0)\n",
    "            self._priors[ind] = X_c.shape[0] / float(n_samples)\n",
    "            \n",
    "        print(\"Mean:\", self._mean)\n",
    "        print(\"Variance:\", self._var)\n",
    "        print(\"Prior\", self._priors)\n",
    "        \n",
    "            \n",
    "    def _calc_pdf(self, class_ind, X) :\n",
    "        mean = self._mean[class_ind]\n",
    "        var = np.diag(self._var[class_ind])\n",
    "        z = np.power(2 * np.pi, X.shape[0]/2) * np.power(np.linalg.det(var), 1/2)\n",
    "        \n",
    "        return (1/z) * np.exp(-0.5 * (X - mean).T @ (np.linalg.inv(var)) @ (X - mean))\n",
    "    \n",
    "\n",
    "    def _calc_prod_likelihood_prior(self, X):\n",
    "        self.q = np.zeros((X.shape[0], len(self._classes)), dtype=np.float64)\n",
    "        \n",
    "        for x_ind, x in enumerate(X) :\n",
    "            for c_ind, c in enumerate(self._classes) :\n",
    "                \n",
    "                # log(pdf) + log(priors) -> Similar to BernoulliNB\n",
    "                self.q[x_ind, c] = (np.log(self._calc_pdf(c_ind, x)) + np.log(self._priors[c_ind]))\n",
    "                \n",
    "    def predict_proba(self, X) :\n",
    "        self._calc_prod_likelihood_prior(X)\n",
    "        return np.exp(self.q) / np.expand_dims(np.sum(np.exp(self.q), axis=1), axis=1)\n",
    "    \n",
    "    def predict(self, X) :\n",
    "        self._calc_prod_likelihood_prior(X)\n",
    "        return np.argmax(self.q, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d91a7705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT : \n",
      "\n",
      "Mean: [[ 4.55327074  5.13550586]\n",
      " [10.33985583 10.04486762]]\n",
      "Variance: [[2.09245276 2.5775431 ]\n",
      " [3.36405042 2.13027877]]\n",
      "Prior [0.5 0.5]\n",
      "None\n",
      "\n",
      "PREDICT : \n",
      "\n",
      "[0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1\n",
      " 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0\n",
      " 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_blobs(n_samples=100, n_features=2, centers=[[5,5],[10,10]], cluster_std=1.5, random_state=2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "gaussian_nb = GaussianNB()\n",
    "print(\"FIT : \\n\")\n",
    "print(gaussian_nb.fit(X, y))\n",
    "print(\"\\nPREDICT : \\n\")\n",
    "print(gaussian_nb.predict(X))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, gaussian_nb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b96f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB():\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "        self.w = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self.w_priors = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_c = X[y == c]\n",
    "            total_count = np.sum(np.sum(X_c, axis=1))\n",
    "            self.w[idx, :] = (np.sum(X_c, axis=0) + self.alpha) / (total_count + n_classes * self.alpha)\n",
    "            self.w_priors[idx] = (X_c.shape[0] + self.alpha) / (float(n_samples) + n_classes * self.alpha)\n",
    "\n",
    "    def log_likelihood_prior_prod(self, X):\n",
    "        return X @ (np.log(self.w).T) + np.log(self.w_priors)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        q = self.log_likelihood_prior_prod(X)\n",
    "        return np.exp(q) / np.expand_dims(np.sum(np.exp(q), axis=1), axis=1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        q = self.log_likelihood_prior_prod(X)\n",
    "        return np.argmax(q, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ea35b",
   "metadata": {},
   "source": [
    "## Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ee401",
   "metadata": {},
   "source": [
    "**PQ1**\n",
    "\n",
    "Write a function euclid(a,b) to find Euclidean distance between vectors a and b. Both a and b have shape (n,1), where n is number of features/dimensions.\n",
    "\n",
    "Input: Vectors a and b\n",
    "Output: Euclidean distance between vectors a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid(a,b) :\n",
    "    return np.sqrt((a-b)**2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1bbc91",
   "metadata": {},
   "source": [
    "**PQ2**\n",
    "\n",
    "Write a function **one_hot(y)** which performs one hot encoding on vector y and then outputs a resultant matrix which can be used for softmax regression as output label matrix. y is row matrix with (n,1) shape, where n is number of samples.\n",
    "Example: if y is [8, 6, 3], its one hot encoding will be [[0, 0, 1],[0, 1, 0],[1, 0, 0]].\n",
    "\n",
    "Input: y: A vector of shape (n,1)\n",
    "Output: A output label matrix of suitable shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59457bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot1(y):\n",
    "    val = set(y)\n",
    "    ei = np.eye(max(val)+1)\n",
    "    return ei[y]\n",
    "\n",
    "\n",
    "# IF WE NEED TO CONVERT IT TO INTEGERS FIRST, USE THE FOLLOWING :\n",
    "\n",
    "def one_hot2(y) :\n",
    "    u = np.unique(y)\n",
    "    idx = np.searchsorted(u, y.flatten())\n",
    "    eye = np.eye(y.shape[0])\n",
    "    return np.row_stack([eye[i] for i in idx.T])\n",
    "    \n",
    "a= np.array([8,6,3])\n",
    "print(one_hot1(a))\n",
    "print(\"\\n\", one_hot2(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee03f3b",
   "metadata": {},
   "source": [
    "**GQ1**\n",
    "Write a function manhattan(a,b) to find Manhattan distance between vectors a and b. Both a and b are vectors with shape (n,1) where n is number of features/dimensions.\n",
    "\n",
    "Input: Vectors a and b\n",
    "Output: Manhattan distance between vectors a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a00f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan(a,b) :\n",
    "    return np.sum(np.abs(a-b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae970e1c",
   "metadata": {},
   "source": [
    "**GQ2**\n",
    "\n",
    "Write a function softmax(Z) to find softmax of linear combination of feature matrix and weight vector.\n",
    "Take care of numerical stability as well.\n",
    "\n",
    "Input: Z = X@W, where X is feature matrix of shape (n,m), W is weight matrix (m,k), where n = number of rows, m = number of features and k = number of labels.\n",
    "\n",
    "Output: A matrix of (n,1) shape. Each row corresponds to the label of that row. Each label will have value from 0 to k-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z) :\n",
    "    s = np.exp(Z) / (np.sum(np.exp(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a4b74",
   "metadata": {},
   "source": [
    "**GQ3**\n",
    "\n",
    "Write a function knn(class1, class2, x_new) to find in which cluster x_new belongs using 3-NN. Assume cluster 1 and cluster 2 has 5 points each. Use Euclidian distance as distance measure.\n",
    "\n",
    "Input: Two numpy arrays class1 and class2 of shape (5,2) each and a numpy array x_new of shape (2,1)\n",
    "Output: Return class id to which x_new belongs (i.e. 1 or 2) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7939a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid(a,b) :\n",
    "    return np.sum((a-b)**2, axis=1).tolist()\n",
    "\n",
    "def knn(class1, class2, x_new) :\n",
    "    c = np.concatenate((class1,class2), axis=0)\n",
    "    cl = []\n",
    "    for i in range(2 * len(class1)) :\n",
    "        if(i < len(class1)) :\n",
    "            cl.append(1)\n",
    "        else :\n",
    "            cl.append(2)\n",
    "    print(c)\n",
    "    points = dict(zip(euclid(c, x_new), cl))    \n",
    "    print(\"All points are \", points)\n",
    "\n",
    "    least_3 = sorted(points.items())[:3]\n",
    "    print(f\"Points nearest to the new examples are {least_3}\")\n",
    "    \n",
    "    return least_3\n",
    "\n",
    "X1 = np.array([[5, 6],[5, 8],[3, 8]]) \n",
    "X2 = np.array([[2,4], [3,3], [4,2]])\n",
    "ne = np.array([2,6])\n",
    "\n",
    "knn(X1, X2, ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63caa5e4",
   "metadata": {},
   "source": [
    "## Week 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a670c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperplane_value(x,w,b, offset):\n",
    "      return -1 * (w[0] * x + b + offset) / w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd411c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fit_softSVM:\n",
    "    def __init__(self, C=500):\n",
    "        \n",
    "        self._support_vectors = None \n",
    "        self.C = C \n",
    "        self.w = None \n",
    "        self.b = None \n",
    "\n",
    "        # n is the number of data points \n",
    "        self.n = 0\n",
    "        # d is the number of dimensions \n",
    "        self.d = 0\n",
    "\n",
    "    def _decision_function(self, X):\n",
    "        return X.dot (self.w) + self.b\n",
    "      \n",
    "    def _cost(self, margin):\n",
    "        return (1 / 2) * self.w.dot (self.w) + self.C * np.sum(np.maximum(0, 1 - margin))\n",
    " \n",
    "    def _margin(self, X, y):\n",
    "        return y * self._decision_function(X)\n",
    " \n",
    "    def fit(self, X, y, lr=0.001, epochs=100):\n",
    "        self.n, self.d = X.shape\n",
    "        self.w = np.random.randn(self.d) \n",
    "        self.b = 0\n",
    "        \n",
    "        loss_array = []\n",
    "        for _ in range ( epochs):\n",
    "            margin = self._margin(X, y) \n",
    "            loss = self._cost(margin) \n",
    "            loss_array.append(loss)\n",
    "            \n",
    "            misclassified_pts_idx = np.where(margin < 1)[0]\n",
    "            d_w = self.w - self.C * y[misclassified_pts_idx].dot(X[misclassified_pts_idx])\n",
    "            self.w = self.w - lr * d_w\n",
    "        \n",
    "            d_b = - self.C * np.sum(y[misclassified_pts_idx])\n",
    "            self.b = self.b - lr * d_b\n",
    "        \n",
    "        self._support_vectors = np.where(self._margin(X, y) <= 1)[0]\n",
    "        return self._support_vectors\n",
    "\n",
    "def support_vectors(X_train, y_train):\n",
    "    softSVM = fit_softSVM()\n",
    "    support_vectors = softSVM.fit(X_train,y_train)\n",
    "\n",
    "    return support_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf2473",
   "metadata": {},
   "source": [
    "## Week 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d758b3",
   "metadata": {},
   "source": [
    "**PQ1**\n",
    "\n",
    "Consider a regression problem with feature matrix X with size (100, 10) and label vector y with size (100, 1). We split this root node into two nodes 'node1' and 'node2' according to jth split variable and split value 's'.\n",
    "Define a function `predict_node(X, y, j, s)` that takes X, y, split variable j and split value s as parameters and returns the tuple of predict values (mean value) at both the nodes.\n",
    "\n",
    "X = ndarray of size (100, 10) with entries as float.\n",
    "\n",
    "y = ndarray of size (100, 1) with entries as float.\n",
    "\n",
    "j = int\n",
    "\n",
    "s = float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_node(X, y, j, s):\n",
    "    sep = X[:, j]\n",
    "    region1 = y[np.where(sep <= s)]\n",
    "    region2 = y[np.where(sep > s)]\n",
    "    \n",
    "    m1 = np.mean(region1)\n",
    "    m2 = np.mean(region2)\n",
    "    \n",
    "    return (m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e42aa",
   "metadata": {},
   "source": [
    "**PQ2**\n",
    "\n",
    "Define a function `predict_class(y)` that takes the parameter\n",
    "`y` which is a ndarray of actual outputs of all the samples in a particualar node and retruns the predict class for the same node.\n",
    "\n",
    "Note: number of classes are 10 (0 to 9). \n",
    "\n",
    "If two classes have same number of samples, function should return the lower class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(y):\n",
    "    classes = np.unique(y)\n",
    "    probs = []\n",
    "    for c in classes :\n",
    "        p = len(np.where(y == c)[0]) / len(y)\n",
    "        probs.append(p)\n",
    "    \n",
    "    pred = classes[np.argmax(probs)]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79416b65",
   "metadata": {},
   "source": [
    "**PQ3**\n",
    "\n",
    "Define a function `misclassification_error(y)`  that takes the parameter `y` which is a ndarray of actual outputs of all the samples in a particular node and return the misclassification error of the same node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2572424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassification_error(y):\n",
    "    classes = np.unique(y)\n",
    "    probs = []\n",
    "    for c in classes :\n",
    "        p = len(np.where(y == c)[0]) / len(y)\n",
    "        probs.append(p)\n",
    "    return 1 - np.max(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d5fe62",
   "metadata": {},
   "source": [
    "**GQ1**\n",
    "\n",
    "Define a function `gini_index(dict)` having the \n",
    "following characteristics:\n",
    "\n",
    "Input:\n",
    "`dict` = dictionary that has classes as keys and 'number of samples in respective class' as values of a particular node.\n",
    "\n",
    "Output:\n",
    "Gini index of the same node. (A float value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353099c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(data):\n",
    "    n = sum(data.values())\n",
    "    ans = 0\n",
    "    for k, v in data.items() :\n",
    "        p = v / n\n",
    "        ans += (p * (1 - p))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3894c8d",
   "metadata": {},
   "source": [
    "**GQ2**\n",
    "\n",
    "Define a function `entropy(dict)` having the \n",
    "following characteristics:\n",
    "\n",
    "Input:\n",
    "`dict` = dictionary that has classes as keys and 'number of samples in respective class' as values of a particular node.\n",
    "\n",
    "Output:\n",
    "Entropy of the same node. (A float value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799bbadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data) :\n",
    "    n = sum(dict.values())\n",
    "    ans = 0\n",
    "    for value in dict.values():\n",
    "        ans += -(value/n)*np.log2(value/n)\n",
    "    return ans  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3d5d8",
   "metadata": {},
   "source": [
    "**GQ3**\n",
    "\n",
    "Consider a regression problem using CART. If y is a ndarray of targets of the samples in a particular node, define a function `sseloss(y)` that returns the error associated with that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32987935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sseloss(y) :\n",
    "    avg = np.mean(y)\n",
    "    res = y - avg\n",
    "    return np.sum((res ** 2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa3bbfa",
   "metadata": {},
   "source": [
    "**Others :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb120add",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ['overcast', 'hot', 'high', 'FALSE', 'yes'],\n",
    "    ['overcast', 'cool', 'normal', 'TRUE', 'yes'],\n",
    "    ['overcast', 'mild', 'high', 'TRUE', 'yes'],\n",
    "    ['overcast', 'hot', 'normal', 'FALSE', 'yes'],\n",
    "    ['rainy', 'mild', 'high', 'FALSE', 'yes'],\n",
    "    ['rainy', 'cool', 'normal', 'FALSE', 'yes'],\n",
    "    ['rainy', 'cool', 'normal', 'TRUE', 'no'],\n",
    "    ['rainy', 'mild', 'normal', 'FALSE', 'yes'],\n",
    "    ['rainy', 'mild', 'high', 'TRUE', 'no'],\n",
    "    ['sunny', 'hot', 'high', 'FALSE', 'no'],\n",
    "    ['sunny', 'hot', 'high', 'TRUE', 'no'],\n",
    "    ['sunny', 'mild', 'high', 'FALSE', 'no'],\n",
    "    ['sunny', 'cool', 'normal', 'FALSE', 'yes'],\n",
    "    ['sunny', 'mild', 'normal', 'TRUE', 'yes']\n",
    "]\n",
    "\n",
    "cols = ['outlook','temp','humidity','windy','play']\n",
    "\n",
    "df = pd.DataFrame(data, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "92ba2f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    hot\n",
       "3    hot\n",
       "Name: temp, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df.keys()[-1]\n",
    "\n",
    "attribute = 'temp'\n",
    "val_attr = 'hot'\n",
    "val_target = 'yes'\n",
    "df[attribute][df[attribute] == val_attr][df[target] == val_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7c876f8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     hot\n",
       "3     hot\n",
       "9     hot\n",
       "10    hot\n",
       "Name: temp, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['temp'][df['temp'] == 'hot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8cb0bec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishvam\\AppData\\Local\\Temp/ipykernel_22752/281809882.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df[df['temp'] == 'hot'][df['play'] == 'yes']['temp']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    hot\n",
       "3    hot\n",
       "Name: temp, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['temp'] == 'hot'][df['play'] == 'yes']['temp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5d92766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['outlook', 'temp', 'humidity', 'windy'], dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_attributes = df.keys()[:-1]\n",
    "all_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "71f5c2bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eps = np.finfo(float).eps\n",
    "\n",
    "def find_entropy_whole(df) :\n",
    "    target = df.keys()[-1]\n",
    "    total_entropy = 0\n",
    "    for t in df[target].unique() :\n",
    "        p = df[target].value_counts()[t] / len(df[target])\n",
    "        entropy = -p * np.log2(p)\n",
    "        print(t, p, entropy)\n",
    "        total_entropy += entropy\n",
    "    return total_entropy\n",
    "\n",
    "def find_entropy_of_attribute(df, attribute) :\n",
    "    target = df.keys()[-1]\n",
    "    entropy_attribute = 0\n",
    "    for a in df[attribute].unique() :\n",
    "        overall_entropy = 0\n",
    "        for t in df[target].unique() :\n",
    "            num = len(df[df[attribute] == a][df[target] == t][attribute])\n",
    "            den = len(df[df[attribute] == a])\n",
    "            \n",
    "            p = num / (den + eps)\n",
    "            overall_entropy += (-p * np.log2(p + eps))\n",
    "            \n",
    "        p2 = den / len(df)\n",
    "        entropy_attribute += -p2 * overall_entropy\n",
    "        \n",
    "    return abs(entropy_attribute)\n",
    "\n",
    "def find_best_attribute_to_divide(df) :\n",
    "    IG = []\n",
    "    all_attributes = df.keys()[:-1]\n",
    "    \n",
    "    for attribute in all_attributes:\n",
    "        IG.append(find_entropy_whole(df) - find_entropy_of_attribute(df, attribute))\n",
    "    index_of_attribute_with_max_IG = np.argmax(IG)\n",
    "    best_attribute = all_attributes[index_of_attribute_with_max_IG]\n",
    "    \n",
    "    return best_attribute\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fb94ef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes 0.6428571428571429 0.40977637753840185\n",
      "no 0.35714285714285715 0.5305095811322292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9402859586706311"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_entropy_whole(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "86d628a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the attribute \"outlook\" is : 0.6935361388961914\n",
      "Entropy of the attribute \"temp\" is : 0.9110633930116756\n",
      "Entropy of the attribute \"humidity\" is : 0.7884504573082889\n",
      "Entropy of the attribute \"windy\" is : 0.892158928262361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishvam\\AppData\\Local\\Temp/ipykernel_22752/3095281830.py:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  num = len(df[df[attribute] == a][df[target] == t][attribute])\n"
     ]
    }
   ],
   "source": [
    "for attribute in df.keys()[:-1]:\n",
    "    print(f'Entropy of the attribute \"{attribute}\" is :', find_entropy_of_attribute(df, attribute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fe2f8c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes 0.6428571428571429 0.40977637753840185\n",
      "no 0.35714285714285715 0.5305095811322292\n",
      "yes 0.6428571428571429 0.40977637753840185\n",
      "no 0.35714285714285715 0.5305095811322292\n",
      "yes 0.6428571428571429 0.40977637753840185\n",
      "no 0.35714285714285715 0.5305095811322292\n",
      "yes 0.6428571428571429 0.40977637753840185\n",
      "no 0.35714285714285715 0.5305095811322292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishvam\\AppData\\Local\\Temp/ipykernel_22752/3095281830.py:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  num = len(df[df[attribute] == a][df[target] == t][attribute])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'outlook'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_attribute_to_divide(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38293cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3,4,5,12,3], [1123,2435,36,47,586,5612,32], [1234,241,635,4765,5678,132,343], [1345,267,5368,84,95,23127,398]])\n",
    "print(a)\n",
    "print(a[:, 4])\n",
    "# print(a[a>2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a673c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTree(df, tree=None) :\n",
    "    target = df.keys()[-1]\n",
    "    node = find_best_attribute_to_divide(df)\n",
    "    attValue = np.unique(df[node])\n",
    "    print(node)\n",
    "    if tree is None :\n",
    "        tree = {}\n",
    "        tree[node] = {}\n",
    "\n",
    "    for value in attValue :\n",
    "        subtree = df[df[node] == value].reset_index(drop=True)\n",
    "        clValue, counts = np.unique(subtree['play'], return_counts=True)\n",
    "\n",
    "        # Checking purity of subset\n",
    "        if len(counts) == 1: \n",
    "            # Leaf Node\n",
    "            tree[node][value] = clValue[0]\n",
    "        else:\n",
    "            # If not a leaf node, call the function recursively\n",
    "            tree[node][value] = buildTree(subtree) \n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce88806",
   "metadata": {},
   "source": [
    "## Week 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f4c54a",
   "metadata": {},
   "source": [
    "**PQ1**\n",
    "\n",
    "Write a function similarity(res,lambda_) which takes residuals at a node as input and returns similarity score which can be used for XGBoost regression.\n",
    "\n",
    "Input: A numpy array having residuals at a node.\n",
    "          lambda is the regularization rate.\n",
    "Output: Similarity score of the node.\n",
    "\n",
    "Note: Similarity Score= (Sum of residuals)2/(No. of residuals+lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2ff702a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(res,lambda_):\n",
    "    return (np.sum(res)**2) / (len(res) + lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c98c91d",
   "metadata": {},
   "source": [
    "**PQ2**\n",
    "\n",
    "Write a function gradboost(model,X_train, y_train, X_test, boosting_rounds,learning_rate) to implement Gradient boost algorithm.\n",
    "\n",
    "Input:\n",
    "model: model to be fitted\n",
    "X_train: Training features\n",
    "y_train: Training output labels\n",
    "X_test: Test feature values\n",
    "boosting_rounds: number of boosting rounds\n",
    "learning_rate: learning rate used in the algorithm\n",
    "\n",
    "Output:\n",
    "y_hat_train: Prediction on training data after number of boosting rounds\n",
    "y_hat_test:  Prediction on testing data after number of boosting rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8cb45d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradboost(model,X_train, y_train, X_test, boosting_rounds,learning_rate) :\n",
    "    \n",
    "    y_train_hat = np.repeat(np.mean(y_train), len(y_train))\n",
    "    y_test_hat = np.repeat(np.mean(y_train), len(y_test))\n",
    "    \n",
    "    residuals = y_train - y_train_hat\n",
    "    \n",
    "    for i in range(boosting_rounds) :\n",
    "        \n",
    "        model = model.fit(X_train, residuals)\n",
    "        \n",
    "        y_train_hat = y_train_hat + (learning_rate * model.predict(X_train))\n",
    "        y_test_hat = y_test_hat + (learning_rate * model.predict(X_test))\n",
    "        \n",
    "        residuals = y_train - y_train_hat\n",
    "        \n",
    "    return y_train_hat, y_test_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4275cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee736d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de1f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
