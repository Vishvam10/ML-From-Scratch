{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b926c6",
   "metadata": {},
   "source": [
    "# Large Scale Machine Learning\n",
    "\n",
    "It differs from traditional ML in the sense that it involves processing a **large amount of data** in terms of its **size** or **number of samples, features or classes**\n",
    "\n",
    "Although scikit is optimized for smaller datasets, it does offer a decent set of feature preprocessing and learning algorithms classification, regression and clustering for large scale data\n",
    "\n",
    "Sckit-learn handles large data using `partial_fit()` method instead of `fit()` method\n",
    "\n",
    "> The idea is to process data in **batches** and **update** the model parameters for each batch. This way of learning is referred to as **incremental (or out-of-core) learning*\n",
    "\n",
    "\n",
    "\n",
    "### Incremental Learning\n",
    "\n",
    "This may be required for the following scenarios :\n",
    "- For **out-of-memory (large) datasets**, where it is not possible to **load the entire data into RAM** at once. One can load the data in chunks and fit the training modell for each chunk of data\n",
    "- For ML tasks where a new batch of data comes with time, re-training the model with the previous and the new batch of data is a computationally expensive process\n",
    "    - So, instead of re-training the model with the entire set of data, one can employ an incremental learning approach, where the model parameters are updated with the new batch of data\n",
    "    \n",
    "    \n",
    "### Incremental Learning in `sklearn`\n",
    "\n",
    "\n",
    "To perform incremental learning, scikit-learn implements `partial_fit()` method. It has the following attributes :\n",
    "\n",
    "- `X` : feature matrix. Shape : (`n_samples`, `n_features`)\n",
    "- `y` : label matrix / vector. Shape : (`n_samples`,)\n",
    "- `classes`: array containing a lis of all the classes that can possibly appear in the y vector. **Must be provided at the first call to `partial_fit()`.** Can be omitted in subsequent calls. Shape : (`n_classes`). \n",
    "- `sample_weight` : (Optional) array containing weights applied to individual sample (1, for unweighted). Shape : (`n_samples`). \n",
    "\n",
    "\n",
    "**To split the dataset into chunks, we can use the `chunksize` parameter in `pd.read_csv()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad23fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b2d466",
   "metadata": {},
   "source": [
    "**Using `fit()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a385883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8939294117647059, 0.8929333333333334)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=50000, n_features=10, n_classes=3, n_clusters_per_class=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "clf1 = SGDClassifier(max_iter=1000, tol=0.01)\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "train_score = clf1.score(X_train, y_train)\n",
    "test_score = clf1.score(X_test, y_test)\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc5e591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85      2488\n",
      "           1       0.98      0.93      0.96      2490\n",
      "           2       0.88      0.87      0.87      2522\n",
      "\n",
      "    accuracy                           0.89      7500\n",
      "   macro avg       0.90      0.89      0.89      7500\n",
      "weighted avg       0.90      0.89      0.89      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d73df7",
   "metadata": {},
   "source": [
    "**Using `partial_fit()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d0b1d49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.69361705, -0.64744075,  0.6422426 , -1.57921405, -0.24399846,\n",
       "        -0.07378696, -2.32927858, -0.02880938, -1.39603102, -1.28222915,\n",
       "         2.        ],\n",
       "       [-0.78335302, -0.81945506, -0.48319819,  0.40735459,  1.03157244,\n",
       "         0.66277752, -1.40071435,  0.69724878, -1.36271671, -1.27478929,\n",
       "         0.        ],\n",
       "       [-0.27796441, -1.24264847, -0.71766529,  0.18127025,  1.16822818,\n",
       "        -0.50003655, -0.72726385, -0.94261259, -0.98844593, -0.29831685,\n",
       "         0.        ],\n",
       "       [-0.05538096, -0.10289486,  0.75766407,  0.38238686, -0.92804648,\n",
       "        -1.03023004, -0.36739391, -0.83933618,  0.17480582,  0.31312676,\n",
       "         2.        ],\n",
       "       [-0.93479936,  0.45279993, -2.16416986,  0.55756503,  2.88121306,\n",
       "         0.26333814,  0.19281967, -0.57317633, -1.15483786,  0.59513994,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.concatenate((X_train, y_train[:, np.newaxis]), axis=1)\n",
    "train_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b3c5a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray(train_data)\n",
    "np.savetxt('train_data.csv', a, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a010ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = SGDClassifier(max_iter=1000, tol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c43e47",
   "metadata": {},
   "source": [
    "Now, read from this file in chunks using pandas read_csv method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "393cc580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunksize = 1000\n",
    "i = 1\n",
    "\n",
    "for train_df in pd.read_csv(\"train_data.csv\", chunksize=chunksize, iterator=True):\n",
    "    X_train_partial = train_df.iloc[:, 0:10]  # Since there are 10 features in dataset\n",
    "    y_train_partial = train_df.iloc[:, 10] # Last column is the label\n",
    "    \n",
    "    # Need to pass the classes in the first iteration, since it's not guaranteed to have the same classes in all chunks of data.\n",
    "    if i == 1:\n",
    "        clf2.partial_fit(X_train_partial, y_train_partial, classes=np.array([0, 1, 2]))\n",
    "    else:\n",
    "        clf2.partial_fit(X_train_partial, y_train_partial)\n",
    "#         print(f'After iteration #{iter}')\n",
    "#         print(clf2.coef_)\n",
    "#         print(clf2.intercept_)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5453074a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishvam\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_score = clf2.score(X_test, y_test)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e6741c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.83      2488\n",
      "           1       0.96      0.93      0.95      2490\n",
      "           2       0.85      0.89      0.87      2522\n",
      "\n",
      "    accuracy                           0.88      7500\n",
      "   macro avg       0.88      0.88      0.88      7500\n",
      "weighted avg       0.88      0.88      0.88      7500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishvam\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf2.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
