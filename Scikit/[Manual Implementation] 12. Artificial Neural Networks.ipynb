{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21fcff9",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4560fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc17c8",
   "metadata": {},
   "source": [
    "## Network Class\n",
    "\n",
    "We shall start writing the `Network` class. The two methods that are indispensable for any ML class are :\n",
    "- `fit`\n",
    "- `predict`\n",
    "\n",
    "Fitting a neural network model requires us to compute two passes on the data :\n",
    "- `forward`\n",
    "- `backward`\n",
    "\n",
    "We need to start at some place by initializing the network and various hyperparameters and this requires an `init` method :\n",
    "- `init`\n",
    "\n",
    "In most of these methods, we would have to take the help of certain helper functions :\n",
    "- `activations`\n",
    "- `losses`\n",
    "\n",
    "This is the process. But we will work through it in the reverse order so that each step of the process does not have any forward references :\n",
    "`helpers -> init -> forward -> backward -> fit -> predict`\n",
    "\n",
    "The skeleton of the class is given in the code block that follows. For ease of exposition, we are going to discuss the methods on at a time and then plugh them into the class right at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878ec7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network :\n",
    "    \n",
    "    def __init__(self, layers, activation_choice=\"relu\", output_choice=\"softmax\", loss_choice=\"cce\") :\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X) :\n",
    "        pass\n",
    "    \n",
    "    def backward(self, Y, Y_hat) :\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, Y, lr=0.01, epochs=100, batch_size=100) :\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X) :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cef7f4",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "### Hidden Layer\n",
    "\n",
    "We will look at 2 functions for the hidden layers. Both of these functions will be **applied element-wise**. The input to these functions can be scalars, vectors or matrices\n",
    "\n",
    "- Sigmoid :\n",
    "$$\n",
    "    g(z) = \\frac {1} {1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Its derivative :\n",
    "\n",
    "$$\n",
    "    g'(z) = g(z)(1 - g(z))\n",
    "$$\n",
    "\n",
    "- ReLU ( Rectified Linear Unit ) :\n",
    "$$\n",
    "    g(x)=\\begin{cases}\n",
    "    z, & z \\ge 0 \\\\ \n",
    "    0, & z<0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Its derivative :\n",
    "\n",
    "$$\n",
    "    g'(x)=\\begin{cases}\n",
    "    1, & z \\ge 0 \\\\ \n",
    "    0, & z<0\n",
    "    \\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008d7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z) :\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def grad_sigmoid(z) :\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def relu(z) :\n",
    "    return np.where(z >= 0, z, 0)\n",
    "\n",
    "def grad_relu(z) :\n",
    "    return np.where(z >= 0, 1, 0)\n",
    "\n",
    "\n",
    "# A dictionary of activation functions will be used while initializing the network\n",
    "hidden_act = {\"sigmoid\": sigmoid, \"relu\": relu}\n",
    "grad_hiddent_act = {\"sigmoid\": grad_sigmoid, \"relu\": grad_relu}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eae455",
   "metadata": {},
   "source": [
    "## Output Layer\n",
    "\n",
    "We will look at 2 activation functions for the output layer :\n",
    "    \n",
    "- Identity ( For regression )\n",
    "$$\n",
    "    g(z) = z\n",
    "$$\n",
    "\n",
    "- Softmax ( For classification ) :\n",
    "The input to the softmax function will always be a matrix of size $n \\times k$. Since we need a probability distribution for each data point, **the softmax will be computed row-wise**\n",
    "\n",
    "$$\n",
    "    g(\\textbf Z) = \n",
    "    \\begin{pmatrix}\n",
    "    ... & ... & ... \\\\\n",
    "    ... & \\frac {e^{Z_{ij}}} {\\sum \\limits_{j=1}^{k} e^{Z_{ij}}} & ... \\\\\n",
    "    ... & ... & ... \\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**To avoid overflow, we will subtract the row-wise maximum from each row while computing the softmax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20273e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(z) :\n",
    "    return z\n",
    "\n",
    "def softmax(z) :\n",
    "    \"\"\"\n",
    "    Row-wise softmax\n",
    "    \"\"\"\n",
    "    # Check if z is a matrix\n",
    "    assert z.ndim == 2\n",
    "    \n",
    "    # To prevent overflow, subtract row-wise maximum\n",
    "    z -= z.max(acis=1, keepdims=True)\n",
    "    \n",
    "    # Compute row-wise softmax\n",
    "    prob = np.exp(z) / np.exp(z).sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Check if each row is a probability distribution\n",
    "    assert np.allclose(prob.sum(axis=1), np.ones(z.shape[0]))\n",
    "    \n",
    "    return prob\n",
    "\n",
    "output_act = {\"softmax\": softmax, \"identity\": identity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0448aa",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "We will use 2 types of losses :\n",
    "\n",
    "- Least Square Loss ( For Regression ) :\n",
    "\n",
    "$$\n",
    "    L(\\hat {\\textbf y}, \\textbf y) = \\frac {1} {2} (\\hat {\\textbf y} - \\textbf y)^{T}(\\hat {\\textbf y} - \\textbf y)\n",
    "$$\n",
    "\n",
    "- Categorial Cross-Entropy Loss ( For Classification ) :\n",
    "    \n",
    "$$\n",
    "    L(\\hat {\\textbf Y}, \\textbf Y) = - \\textbf 1_{n}^T(\\textbf Y \\odot log(\\hat {\\textbf Y})\\textbf1-{k}\n",
    "$$\n",
    "\n",
    "In our implementation, we will assume that the arguments to the loss function are always matrices of size $n \\times k$. In the case of regression, $k = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f78505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lease_square(y, y_hat) :\n",
    "    return 0.5 * np.sum((y_hat - y) * (y_hat - y))\n",
    "\n",
    "def cce(Y, Y_hat) :\n",
    "    return -np.sum(Y * np.log(Y_hat))\n",
    "\n",
    "losses = {\"least_squares\": least_squares, \"cce\": cce}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ebe5a",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Here, we will look at two parts :\n",
    "\n",
    "### Network architecture\n",
    "\n",
    "The following components mainly determine the structure of the network :\n",
    "- Number of layers\n",
    "- Number of neuron per layer\n",
    "We will use $l$ to index the layers. The network has $L$ layers in all.\n",
    "- $l = 0$ : Input layer\n",
    "- $1 \\le l \\le L - 1$ : Hidden layers\n",
    "- $l = L$ : Output layer\n",
    "\n",
    "We shall represent the number of layers and neurons using a list `layers`. The variable $L$ will never make an explicit appearance anywhere, instead will use `range(len(layers))` to iterate through the layers.\n",
    "\n",
    "| Layer           | Number of neurons |\n",
    "| -----------     | -----------       |\n",
    "| Input layer     | `layers[0]`       |\n",
    "| Hidden layer 1  | `layers[1]`       |\n",
    "| Hidden layer 2  | `layers[2]`       |\n",
    "| Hidden layer 3  | `layers[3]`       |\n",
    "| **.......**     | `layers[...]`     |\n",
    "| Output layer    | `layers[-1]`      |\n",
    "\n",
    "One useful task is to compute the total number of parameters in each network. This will come handy later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc88cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(layers) :\n",
    "    num_params = 0\n",
    "    for l in range(1, len(layers)) :\n",
    "        num_weights = layers[l - 1] * layers[l]\n",
    "        num_biases = layers[l]\n",
    "        num_params += (num_weights + num_biases)\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783ed08",
   "metadata": {},
   "source": [
    "### Parameter initialization\n",
    "\n",
    "- The weight matrix at layer $l$ has a size of `layers[l - 1] x layers[l]`\n",
    "- The bias at layer $l$ is a vector of size `layers[l]`\n",
    "- We will store all these weights in a list `w` of the same size as `layers`. So, `W[l]` will correspond to $\\textbf W_l$. Since there are $L$ weight matrices `W[0]` would be set to `None`. Recall that size of the list if $L + 1$\n",
    "- A similar list would be required for `b`\n",
    "\n",
    "To make the gradient descent update simpler, it will be useful to have a **master vector**, $\\textbf \\theta$, that has a refernce to all the parameters in the network. We will do the same for the gradients $\\textbf \\theta^{(g)}$. So, whenever $\\textbf \\theta$ is updated, the weights $\\textbf W_l$ will also be updated and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb8f8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf35dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393cf065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a277ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91273e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2dbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ead4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9c4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166e866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3c278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79026ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1dab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b336f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aed350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad96e150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1838f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093291c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d471e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2714e649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
